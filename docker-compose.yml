version: "3.9"

services:
  trident:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - type: bind
        source: ${LOCAL_WORKSPACE_FOLDER:-.}
        target: /app
    command: npm run dev
    ports:
      - 3000:3000
    networks:
      - myapp
  llama-cpp-server:
    image: local/llama.cpp:full
    ports:
      - 8080:8080
    networks:
      - myapp
    tty: true
    command: >
      --server
      --model /models/${MODEL_NAME:-tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf}
      --ctx-size ${CONTEXT_SIZE:-2048}
      --threads ${THREADS:-8}
      --mlock --no-mmap --embedding
      --port 8080 --host 0.0.0.0
    volumes:
      - ~/llama.cpp/models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
  overpass-api:
    # MEMO:
    # http://localhost:5000/api/interpreter?data=[out:json];rel(2220322);out;
    image: wiktorn/overpass-api
    ports:
      - 5000:80
    volumes:
      - overpass-db:/db
    environment:
      - OVERPASS_META=no
      - OVERPASS_MODE=init
      - OVERPASS_PLANET_URL=http://download.geofabrik.de/europe/monaco-latest.osm.bz2
      - OVERPASS_DIFF_URL=http://download.openstreetmap.fr/replication/europe/monaco/minute/
      - OVERPASS_UPDATE_SLEEP=60
      - OVERPASS_USE_AREAS=true
      - OVERPASS_STOP_AFTER_INIT=false
      - OVERPASS_MAX_TIMEOUT=30000

networks:
  myapp:
    name: myapp

volumes:
  overpass-db:
